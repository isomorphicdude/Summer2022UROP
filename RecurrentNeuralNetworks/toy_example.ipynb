{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A toy example  \n",
    "\n",
    "In this notebook, we will use vanilla LSTM recurrent neural networks to learn our model.  \n",
    "\n",
    "*Note: In this notebook, we will use the tensorflow probability library, which needs to be installed as it's not part of tensorflow*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import sys; sys.path.insert(0, '..')\n",
    "from data.data_generator import *\n",
    "from preprocess import *\n",
    "from window_data import *\n",
    "\n",
    "tfpl = tfp.layers\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data  \n",
    "\n",
    "We get data using the first model (also the simplest). We also only use `10` samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to use multiprocessing...\n",
      "Multiprocessing successful.\n",
      "Time taken: 1.225876808166504 seconds.\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "total = generateData(model1,\n",
    "        num_data = 10,\n",
    "        init_sty = 'random',\n",
    "        times = (0, 20),\n",
    "        params = {'no. of prey': N, \n",
    "    'kappa for prey': 0.5, \n",
    "    'attraction of prey a': 1, \n",
    "    'repulsion of prey b_1': 1, \n",
    "    'repulsion of pred b_2': 0.07, \n",
    "    'attraction of pred c': 10, \n",
    "    'exponent of dist pred p': 1.2},\n",
    "        steps = 1000,\n",
    "        second_order = False,\n",
    "        method = 'rk2',\n",
    "        return_vel = False,\n",
    "        cores = 8,\n",
    "        flattened=False)\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end-start} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot showing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiPlot([total[0][1], 20/1000, 10], sample_points =[0,0.5,2,4,6,8,10],\n",
    "#             axis_lim = None, second_order = False, quiver=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has the shape `(batch, times, individuals, coordinates)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1000, 21, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([total[i][1] for i in range(len(total))])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only use one initial condition for this experiment, as our naive implementation only works well with one time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 21, 2) (100, 21, 2) (100, 21, 2)\n"
     ]
    }
   ],
   "source": [
    "data = data[0]\n",
    "train_ds, valid_ds, test_ds = getDatasets(data, scaling = False, return_ndarray=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an experiment, change to `input_width=900, label_width=100, shift=100`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total window size: 15\n",
      "Input indices: [0 1 2 3 4 5 6 7 8 9]\n",
      "Label indices: [10 11 12 13 14]\n",
      "Label start: 10\n",
      "Input slice: slice(0, 10, None)\n",
      "Label slice: slice(10, None, None)\n"
     ]
    }
   ],
   "source": [
    "window1 = WindowData(input_width=10, label_width=5, shift=5,\n",
    "                    train_ds=train_ds, val_ds=valid_ds, test_ds=test_ds)\n",
    "print(window1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n",
      "(TensorSpec(shape=(None, 10, 21, 2), dtype=tf.float32, name=None), TensorSpec(shape=(None, 5, 21, 2), dtype=tf.float32, name=None))\n",
      "21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 23:50:49.859946: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-09-14 23:50:49.860048: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "train_ds = window1.make_train()\n",
    "valid_ds = window1.make_val()\n",
    "test_ds = window1.make_test()\n",
    "\n",
    "print(train_ds.element_spec)\n",
    "print(window1.num_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A naive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is that for data of shape `(batch, times, individuals)`, we pass to an LSTM layer after `embedding` it in some way (the idea is similar to one-hot encoding of integer/categorical values), where it has output shape `(batch, times, length of concatenated embeddings)`, we can then produce a prediction at a single future time step of the shape `(batch, 1, individuals)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedder (embedder)         (32, 10, 1344)            861504    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 861,504\n",
      "Trainable params: 861,504\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from rnn import *\n",
    "\n",
    "model = tf.keras.Sequential([tf.keras.layers.Input(shape=(10,21,2)),\n",
    "                            embedder((10,21,2), 64, batch_size=32)])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=float64, numpy=\n",
       "array([[ 1.31127106,  0.        ,  0.        ,  0.        ],\n",
       "       [-0.46805656, -1.09843779,  0.        ,  0.        ],\n",
       "       [ 0.99597515, -0.07251197,  0.03541874,  0.        ],\n",
       "       [ 1.08537411,  0.17744395,  0.07478798,  0.46986297]])>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 4\n",
    "coeffs = tf.constant(np.random.normal(0, 1, int(n*(n+1)/2)), dtype=tf.float64)\n",
    "lower_diag = tfp.math.fill_triangular(coeffs)\n",
    "lower_diag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedder_9 (embedder)       (32, 10, 1344)            861504    \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (32, 128)                 754176    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (32, 525)                 67725     \n",
      "                                                                 \n",
      " reshape_7 (Reshape)         (32, 5, 21, 5)            0         \n",
      "                                                                 \n",
      " distribution_lambda (Distri  ((32, 5, 21, 2),         0         \n",
      " butionLambda)                (32, 5, 21, 2))                    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,683,405\n",
      "Trainable params: 1,683,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 64\n",
    "\n",
    "lstm_model_1 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input((10,21,2)),\n",
    "    embedder((10,21,2), embedding_size, batch_size=32),\n",
    "    tf.keras.layers.LSTM(2*embedding_size, return_sequences=False),\n",
    "    # 5 outputs for each trajectory as it's bivariate normal\n",
    "    # there are window1.num_points trajectories (21 in this case)\n",
    "    # so we need 5*21 outputs at each time step\n",
    "    # there are window1.label_width time steps (5 in this case)\n",
    "    # so we have 5*21*5 outputs from Dense layer\n",
    "    # first two 21 blocks are means, last 21*3 block \n",
    "    # form the lower-tril matrix (consecutive 3 for each coord)\n",
    "    # final output shape should be (batch, 5, 21, 2)\n",
    "    tf.keras.layers.Dense(5*window1.num_points*5, activation='linear'),\n",
    "    tf.keras.layers.Reshape((5, window1.num_points, 5)),\n",
    "    # the loc should be (5, 21, 2)\n",
    "    # the scale_tril should be (5, 21, 2, 2)\n",
    "    tfpl.DistributionLambda(lambda x: tfd.MultivariateNormalTriL(\n",
    "                            loc=x[..., :2], \n",
    "                            scale_tril=tfp.math.fill_triangular(x[...,2:])\n",
    "                            )\n",
    "                           )\n",
    "])\n",
    "\n",
    "lstm_model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going on, we take a sample from our dataset and pass it through the model to verify the output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log probability shape is:\n",
      "(32, 5, 21)\n",
      "The true value's shape is:\n",
      "(32, 5, 21, 2)\n",
      "After reduction the probability shape is:\n",
      "(32, 21)\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_ds.take(1):\n",
    "    print(\"The log probability shape is:\")\n",
    "    print(lstm_model_1(x).log_prob(y).shape)\n",
    "    print(\"The true value's shape is:\")\n",
    "    print(y.shape)\n",
    "    print(\"After reduction the probability shape is:\")\n",
    "    print(tf.reduce_mean(lstm_model_1(x).log_prob(y), axis=1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define also a custom loss function that computes the negative log likelihood over the time steps of prediction.  \n",
    "\n",
    "Recall that we would like to sum over the prediction time steps, which is the second axis in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negLog(y_true, y_pred):\n",
    "    return -tf.reduce_sum(y_pred.log_prob(y_true), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `RMSProp` as in the paper to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model_1.compile(loss=negLog, optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.003))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model_1.fit(train_ds, epochs=10, validation_data=valid_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c63bf7c73efbaa60a9891fdddd1e96dd0cc596469d20228077150d640c222586"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
